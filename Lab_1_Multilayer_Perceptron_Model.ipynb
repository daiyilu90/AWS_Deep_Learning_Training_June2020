{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Create and run a multilayer perceptron model\n",
    "\n",
    "In this task, you create neural networks without using abstracted methods from powerful ML libraries like [Apache MXNet](https://mxnet.apache.org/). However, you will still use some basic functions from MXNet that make it easier to model these neural networks.\n",
    "\n",
    "You will focus on the problem of multi-class classification, which can be expressed as the simplest neural network easily. You will create a dummy dataset where the results (labels) are known. You will use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), which consists of centrally cropped, black and white photographs of handwritten digits. Each image is 28x28 pixels:\n",
    "\n",
    "![](mnistdigits.gif)\n",
    "\n",
    "You will create a four-layer neural network to detect digits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run each cell in this notebook by pressing **SHIFT + ENTER**. When the cell finishes running, the text to the left of the cell changes from from **In [*]:** to **In [1]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd\n",
    "print(\"Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, specify using a GPU with MXNet by running the next cell. To use a CPU instead, before running the cell below, uncomment the CPU line and comment out the GPU line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a GPU with MXNet\n",
    "ctx = mx.gpu()\n",
    "\n",
    "# Use a CPU with MXNet\n",
    "# ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the [MNIST image dataset](http://yann.lecun.com/exdb/mnist/) for a multilayer perceptron neural network. Download the dataset by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MNIST image dataset\n",
    "mnist = mx.test_utils.get_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters for your neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of inputs: 1-dimensional input consisting of a single image (28x28 pixels)\n",
    "num_inputs = 784\n",
    "\n",
    "# Number of outputs: Number of outputs to be predicted by the network (digits 0-9)\n",
    "num_outputs = 10\n",
    "\n",
    "# Batch size: Number of images processed in a single batch\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, split the dataset into training and test data. Use the Gluon API to use a DataLoader to iterate through the dataset in mini-batches. A DataLoader is used to create mini-batches of samples from a Dataset and provides a convenient iterator interface for looping these batches. Itâ€™s typically much more efficient to pass a mini-batch of data through a neural network than a single sample at a time, because the computation can be performed in parallel.\n",
    "\n",
    "Gluon provides a convenient API to download the MNIST dataset using `mxnet.gluon.data.vision.MNIST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training data and test data\n",
    "\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define two useful parameters: **number of hidden neurons** and **weights scale**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hidden neurons\n",
    "num_hidden = 128\n",
    "\n",
    "# Weights scale\n",
    "weight_scale = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can train the network, you need to define the layers. Start by defining the parameters (**weights** and **bias**) for the first layer.\n",
    "\n",
    "<i class=\"fas fa-comment\"></i> The next cell might take some time to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate weights and bias for the first layer\n",
    "w_hd_1 = nd.random_normal(shape=(num_inputs, num_hidden), scale=weight_scale, ctx=ctx)\n",
    "b_hd_1 = nd.random_normal(shape=num_hidden, scale=weight_scale, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameters for the second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate weights and bias for the second layer\n",
    "w_hd_2 = nd.random_normal(shape=(num_hidden, num_hidden), scale=weight_scale, ctx=ctx)\n",
    "b_hd_2 = nd.random_normal(shape=num_hidden, scale=weight_scale, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameters for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate weights and bias for the output layer\n",
    "w_output = nd.random_normal(shape=(num_hidden, num_outputs), scale=weight_scale, ctx=ctx)\n",
    "b_output = nd.random_normal(shape=num_outputs, scale=weight_scale, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the parameters to a list to be able to calculate the gradients on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parameters to calculate gradients\n",
    "params = [w_hd_1, b_hd_1, w_hd_2, b_hd_2, w_output, b_output]\n",
    "\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compose a multilayer network but use only linear operations, then your entire network will still be a linear function. That's because $\\hat{y} = X \\cdot W_1 \\cdot W_2 \\cdot W_2 = X \\cdot W_4 $ for $W_4 = W_1 \\cdot W_2 \\cdot W3$. To give the model the capacity to capture nonlinear functions, you need to interleave the linear operations with activation functions. In this case, you will use the rectified linear unit (ReLU).\n",
    "\n",
    "To define a ReLU activation function for the hidden layer, run the following cell.\n",
    "\n",
    "You can also use other activation functions like sigmoid or tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a ReLU activation function for the hidden layer\n",
    "def relu(X):\n",
    "    return nd.maximum(X, nd.zeros_like(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax output\n",
    "\n",
    "For the output of the network, the predictions are an array that predicts digits 0-9. The softmax action function for the output layer gived you the probabilities of an image being of a particular class. For example, if the first number in the array is 0.65, this means there is a 65% probability that the number is 0.\n",
    "\n",
    "Instead of passing softmax probabilities into the new loss function, just pass the yhat_linear and compute the softmax and its log all at once inside the softmax_cross_entropy loss function, which does smart things like the LogSumExp trick ([LogSumExp on Wikipedia](https://en.wikipedia.org/wiki/LogSumExp)). This solves some numerical instability issues that might arise due to exploding or vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a softmax action function for the output layer\n",
    "def softmax_cross_entropy(yhat_linear, y):\n",
    "    return - nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Define an artificial neural network model\n",
    "\n",
    "In this task, you will define an artificial neural network, an optimizer to learn the weights and biases, and the evaluation metric to evaluate how your model is doing.\n",
    "\n",
    "To define the model, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "def net(X):\n",
    "\n",
    "    #  Compute the first hidden layer\n",
    "    h1_linear = nd.dot(X, w_hd_1) + b_hd_1\n",
    "    h1 = relu(h1_linear)\n",
    "\n",
    "    #  Compute the second hidden layer\n",
    "    h2_linear = nd.dot(h1, w_hd_2) + b_hd_2\n",
    "    h2 = relu(h2_linear)\n",
    "\n",
    "    #  Compute the output layer\n",
    "    yhat_linear = nd.dot(h2, w_output) + b_output\n",
    "    return yhat_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned variable is still a linear variable that does not have a softmax function applied to it. This function is directly applied in the softmax_cross_entropy to prevent numerical stability issues that might arise during backpropagation.\n",
    "\n",
    "Define an optimizer to learn the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation metric\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        numerator += nd.sum(predictions == label)\n",
    "        denominator += data.shape[0]\n",
    "    return (numerator / denominator).asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Run the training loop and evaluate the model\n",
    "\n",
    "In this task, you will run the training loop and evaluate the model.\n",
    "\n",
    "Define the parameters for executing the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs: Iterations over the full network\n",
    "epochs = 10\n",
    "\n",
    "# Learning rate: Speed at which the network learns\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define a smooth constant for the moving loss\n",
    "smoothing_constant = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your artificial neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network model\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Output:**\n",
    "\n",
    "Epoch 0. Loss: 0.462392371464, Train_acc 0.8805, Test_acc 0.8831 \n",
    "\n",
    "Epoch 1. Loss: 0.285959471388, Train_acc 0.919967, Test_acc 0.9194\n",
    "\n",
    "Epoch 2. Loss: 0.198550129106, Train_acc 0.94725, Test_acc 0.9499\n",
    "\n",
    "Epoch 3. Loss: 0.159744916748, Train_acc 0.9602, Test_acc 0.958\n",
    "\n",
    "Epoch 4. Loss: 0.125638222475, Train_acc 0.967033, Test_acc 0.9619\n",
    "\n",
    "Epoch 5. Loss: 0.101477091803, Train_acc 0.97465, Test_acc 0.9689\n",
    "\n",
    "Epoch 6. Loss: 0.0901461152782, Train_acc 0.976233, Test_acc 0.9693\n",
    "\n",
    "Epoch 7. Loss: 0.0763436301528, Train_acc 0.98115, Test_acc 0.9737\n",
    "\n",
    "Epoch 8. Loss: 0.0693615894988, Train_acc 0.9841, Test_acc 0.9745\n",
    "\n",
    "Epoch 9. Loss: 0.0573878861228, Train_acc 0.985933, Test_acc 0.9739\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Pick a few random data points from the test set to visualize alongside the predictions. Quantitatively, the model is more accurate, but visualizing results is a good practice because it provides:\n",
    "* A sanity check that the code is actually working\n",
    "* Intuition about what kinds of mistakes the model tends to make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function to do prediction\n",
    "def model_predict(net,data):\n",
    "    output = net(data)\n",
    "    return nd.argmax(output, axis=1)\n",
    "\n",
    "samples = 10\n",
    "\n",
    "mnist_test = mx.gluon.data.vision.MNIST(train=False, transform=transform)\n",
    "\n",
    "# let's sample 10 random data points from the test set\n",
    "sample_data = mx.gluon.data.DataLoader(mnist_test, samples, shuffle=True)\n",
    "for i, (data, label) in enumerate(sample_data):\n",
    "    data = data.as_in_context(ctx)\n",
    "    im = nd.transpose(data,(1,0,2,3))\n",
    "    im = nd.reshape(im,(28,10*28,1))\n",
    "    imtiles = nd.tile(im, (1,1,3))\n",
    "    \n",
    "    plt.imshow(imtiles.asnumpy())\n",
    "    plt.show()\n",
    "    pred=model_predict(net,data.reshape((-1,784)))\n",
    "    print('model predictions are:', pred)\n",
    "    print('true labels :', label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how your model does on unseen data by running the following cell. This downloads an HTML page containing a canvas for you to draw in the Jupyter notebook.\n",
    "\n",
    "Jupyter notebooks contain built-in magic commands that let you run bash commands and others in a notebook cell. For more information, see [Magic Commands](http://ipython.readthedocs.io/en/stable/interactive/magics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget http://us-west-2-tcprod.s3.amazonaws.com/courses/ILT-TF-200-MLDEEP/v1.5.1/lab-1-setup-sagemaker/scripts/mnist.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create an HTML canvas to evaluate to the model.\n",
    "\n",
    "Use your mouse to draw a digit on the canvas, and then click **Classify**.\n",
    "\n",
    "Try writing every digit and see how it goes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an HTML canvas to evaluate the model\n",
    "from IPython.display import HTML\n",
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "\n",
    "def classify(img):\n",
    "    img = base64.b64decode(img[len('data:image/png;base64,'):])\n",
    "    img = cv2.imdecode(np.fromstring(img, np.uint8),-1)\n",
    "    img = cv2.resize(img[:,:,3], (28,28))\n",
    "    img = nd.array(img).as_in_context(ctx).reshape((-1, 784)).astype(np.float32)/255\n",
    "    return int(nd.argmax(net(img), axis=1).asnumpy()[0])\n",
    "\n",
    "HTML(filename = \"mnist.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "- Try changing the network size to 5 layers.\n",
    "- Try changing the number of hidden units to 256."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab complete\n",
    "\n",
    "Congratulations! You have completed this lab. To clean up your lab environment, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Log out of Jupyter Notebook by clicking **Quit**. Then, close the tab.\n",
    "- Log out of the AWS Management Console by clicking **awsstudent** at the top of the console, and then clicking **Sign Out**.\n",
    "- End the lab session in Qwiklabs by clicking **End Lab**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
